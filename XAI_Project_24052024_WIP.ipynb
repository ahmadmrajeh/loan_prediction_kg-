{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "SZaa2CoHRQsN",
        "outputId": "8bc15463-3f2d-45ee-8d1d-9543d016d6c4"
      },
      "id": "SZaa2CoHRQsN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load raw data to env\n",
        "!unzip \"/content/drive/MyDrive/Copy of home-credit-default-risk.zip\" -d \"./DataSet/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8PM_0SdRsKH",
        "outputId": "187195f5-0a3f-4062-ce67-77bad88911f4"
      },
      "id": "M8PM_0SdRsKH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Copy of home-credit-default-risk.zip\n",
            "  inflating: ./DataSet/HomeCredit_columns_description.csv  \n",
            "  inflating: ./DataSet/POS_CASH_balance.csv  \n",
            "  inflating: ./DataSet/application_test.csv  \n",
            "  inflating: ./DataSet/application_train.csv  \n",
            "  inflating: ./DataSet/bureau.csv    \n",
            "  inflating: ./DataSet/bureau_balance.csv  \n",
            "  inflating: ./DataSet/credit_card_balance.csv  \n",
            "  inflating: ./DataSet/installments_payments.csv  \n",
            "  inflating: ./DataSet/previous_application.csv  \n",
            "  inflating: ./DataSet/sample_submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFgSB82CVsH2",
        "outputId": "cde279e7-0823-4977-8d86-2cbd56f7fdae"
      },
      "id": "yFgSB82CVsH2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdflib import Graph, Namespace, URIRef, Literal\n",
        "from rdflib.namespace import RDF, RDFS, XSD\n",
        "import urllib.parse"
      ],
      "metadata": {
        "id": "DRMidHMGKuT_"
      },
      "id": "DRMidHMGKuT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdflib import Graph, Namespace, Literal, RDF, RDFS, URIRef\n",
        "from rdflib.namespace import XSD"
      ],
      "metadata": {
        "id": "YWx7U-trW10Z"
      },
      "id": "YWx7U-trW10Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use this Code (combined effort of the Team!)"
      ],
      "metadata": {
        "id": "Om5DHd4JgTkH"
      },
      "id": "Om5DHd4JgTkH"
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging Data\n",
        "import pandas as pd\n",
        "from rdflib import Graph, Namespace, URIRef, Literal\n",
        "from rdflib.namespace import RDF, RDFS, XSD\n",
        "import urllib.parse\n",
        "\n",
        "# Adjust sampling fraction as needed\n",
        "sampling_fraction = 0.1\n",
        "\n",
        "# Load and sample data files\n",
        "application_train = pd.read_csv('/content/DataSet/application_train.csv')\n",
        "application_train = application_train.dropna(subset=['SK_ID_CURR', 'TARGET', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS'])\n",
        "application_train=application_train[['SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
        "    'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n",
        "    'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n",
        "    'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'OWN_CAR_AGE',\n",
        "    'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'TARGET']]\n",
        "application_train = application_train.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "bureau = pd.read_csv('/content/DataSet/bureau.csv', usecols=[\n",
        "    'SK_ID_CURR', 'SK_ID_BUREAU', 'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT',\n",
        "    'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE',\n",
        "    'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT',\n",
        "    'AMT_CREDIT_SUM_OVERDUE', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE'\n",
        "])\n",
        "bureau = bureau.dropna(subset=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
        "bureau = bureau.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "bureau_balance = pd.read_csv('/content/DataSet/bureau_balance.csv')\n",
        "bureau_balance = bureau_balance.dropna(subset=['SK_ID_BUREAU'])\n",
        "bureau_balance = bureau_balance.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "credit_card_balance = pd.read_csv('/content/DataSet/credit_card_balance.csv', usecols=[\n",
        "    'SK_ID_CURR', 'SK_ID_PREV', 'AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL',\n",
        "    'AMT_DRAWINGS_ATM_CURRENT', 'AMT_DRAWINGS_CURRENT', 'AMT_DRAWINGS_OTHER_CURRENT',\n",
        "    'AMT_DRAWINGS_POS_CURRENT', 'AMT_INST_MIN_REGULARITY', 'AMT_PAYMENT_CURRENT',\n",
        "    'AMT_PAYMENT_TOTAL_CURRENT', 'AMT_RECEIVABLE_PRINCIPAL', 'AMT_RECIVABLE',\n",
        "    'AMT_TOTAL_RECEIVABLE'\n",
        "])\n",
        "credit_card_balance = credit_card_balance.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "credit_card_balance = credit_card_balance.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "installments_payments = pd.read_csv('/content/DataSet/installments_payments.csv', usecols=[\n",
        "    'SK_ID_CURR', 'SK_ID_PREV', 'AMT_INSTALMENT', 'NUM_INSTALMENT_VERSION',\n",
        "    'NUM_INSTALMENT_NUMBER', 'DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT', 'AMT_PAYMENT'\n",
        "])\n",
        "installments_payments = installments_payments.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "installments_payments = installments_payments.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "pos_cash_balance = pd.read_csv('/content/DataSet/POS_CASH_balance.csv', usecols=[\n",
        "    'SK_ID_CURR', 'SK_ID_PREV', 'CNT_INSTALMENT', 'CNT_INSTALMENT_FUTURE', 'SK_DPD', 'SK_DPD_DEF'\n",
        "])\n",
        "pos_cash_balance = pos_cash_balance.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "pos_cash_balance = pos_cash_balance.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "previous_application = pd.read_csv('/content/DataSet/previous_application.csv', usecols=[\n",
        "    'SK_ID_CURR', 'SK_ID_PREV', 'AMT_APPLICATION', 'DAYS_DECISION', 'NAME_PAYMENT_TYPE',\n",
        "    'NAME_CLIENT_TYPE', 'NAME_PRODUCT_TYPE', 'NAME_SELLER_INDUSTRY', 'AMT_DOWN_PAYMENT',\n",
        "    'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START'\n",
        "])\n",
        "previous_application = previous_application.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "previous_application = previous_application.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "# Merge dataframes with specified suffixes to avoid column name conflicts\n",
        "merged_df = application_train.copy()\n",
        "merged_df = merged_df.merge(bureau, on='SK_ID_CURR', how='left', suffixes=('', '_bureau'))\n",
        "merged_df = merged_df.merge(bureau_balance, on='SK_ID_BUREAU', how='left', suffixes=('', '_bureau_balance'))\n",
        "merged_df = merged_df.merge(credit_card_balance, on='SK_ID_CURR', how='left', suffixes=('', '_credit_card'))\n",
        "merged_df = merged_df.merge(installments_payments, on='SK_ID_CURR', how='left', suffixes=('', '_installments'))\n",
        "merged_df = merged_df.merge(pos_cash_balance, on='SK_ID_CURR', how='left', suffixes=('', '_pos_cash'))\n",
        "merged_df = merged_df.merge(previous_application, on='SK_ID_CURR', how='left', suffixes=('', '_previous'))\n",
        "\n",
        "merged_df.to_csv('/content/DataSet/merged_sampled_application_data_2.csv', index=False)"
      ],
      "metadata": {
        "id": "2GCZnuH5s7rv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "87469de7-5239-40a5-cf56-68e019cc0bfb"
      },
      "id": "2GCZnuH5s7rv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/DataSet/application_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-00b3cc557b76>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load and sample data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mapplication_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/DataSet/application_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mapplication_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplication_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SK_ID_CURR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TARGET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CODE_GENDER'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_OWN_CAR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_OWN_REALTY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNT_CHILDREN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_INCOME_TOTAL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_CREDIT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_ANNUITY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_GOODS_PRICE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_TYPE_SUITE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_INCOME_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_EDUCATION_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_FAMILY_STATUS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_HOUSING_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REGION_POPULATION_RELATIVE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_BIRTH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_EMPLOYED'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_REGISTRATION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_ID_PUBLISH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OWN_CAR_AGE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_MOBIL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_WORK_PHONE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_PHONE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_EMAIL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OCCUPATION_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNT_FAM_MEMBERS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m application_train=application_train[['SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/DataSet/application_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#merged_df=pd.read_csv(\"/content/DataSet/merged_sampled_application_data_2.csv\")\n",
        "\n",
        "# Define namespaces\n",
        "ns = Namespace(\"http://example.org/ontology#\")\n",
        "RDF_NS = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "RDFS_NS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "\n",
        "# Create a new RDF graph\n",
        "g = Graph()\n",
        "g.bind(\"ex\", ns)\n",
        "g.bind(\"rdf\", RDF_NS)\n",
        "g.bind(\"rdfs\", RDFS_NS)\n",
        "\n",
        "# Define classes and properties from the T-Box schema\n",
        "g.add((ns.Applicant, RDF.type, RDFS.Class))\n",
        "g.add((ns.AgeGroup, RDF.type, RDFS.Class))\n",
        "g.add((ns.Bureau, RDF.type, RDFS.Class))\n",
        "g.add((ns.CreditCard, RDF.type, RDFS.Class))\n",
        "g.add((ns.IncomeGroup, RDF.type, RDFS.Class))\n",
        "g.add((ns.IncomeSource, RDF.type, RDFS.Class))\n",
        "g.add((ns.Loan, RDF.type, RDFS.Class))\n",
        "g.add((ns.LoanApplication, RDF.type, RDFS.Class))\n",
        "g.add((ns.Payment, RDF.type, RDFS.Class))\n",
        "g.add((ns.ExternalDataSource, RDF.type, RDFS.Class))\n",
        "\n",
        "g.add((ns.appliedFor, RDF.type, RDF.Property))\n",
        "g.add((ns.appliedFor, RDFS.domain, ns.LoanApplication))\n",
        "g.add((ns.appliedFor, RDFS.range, ns.Loan))\n",
        "\n",
        "g.add((ns.appliedPreviously, RDF.type, RDF.Property))\n",
        "g.add((ns.appliedPreviously, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.appliedPreviously, RDFS.range, ns.LoanApplication))\n",
        "\n",
        "# Define LoanStatus class and its subclasses Default and NotDefault\n",
        "g.add((ns.LoanStatus, RDF.type, RDFS.Class))\n",
        "g.add((ns.Default, RDF.type, ns.LoanStatus))\n",
        "g.add((ns.NotDefault, RDF.type, ns.LoanStatus))\n",
        "\n",
        "g.add((ns.classifiedAs, RDF.type, RDF.Property))\n",
        "g.add((ns.classifiedAs, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.classifiedAs, RDFS.range, ns.LoanStatus))\n",
        "\n",
        "g.add((ns.hasAgeGroup, RDF.type, RDF.Property))\n",
        "g.add((ns.hasAgeGroup, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasAgeGroup, RDFS.range, ns.AgeGroup))\n",
        "\n",
        "g.add((ns.hasCreditCard, RDF.type, RDF.Property))\n",
        "g.add((ns.hasCreditCard, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasCreditCard, RDFS.range, ns.CreditCard))\n",
        "\n",
        "g.add((ns.hasCreditHistory, RDF.type, RDF.Property))\n",
        "g.add((ns.hasCreditHistory, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasCreditHistory, RDFS.range, ns.Bureau))\n",
        "\n",
        "g.add((ns.hasExistingLoan, RDF.type, RDF.Property))\n",
        "g.add((ns.hasExistingLoan, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasExistingLoan, RDFS.range, ns.Loan))\n",
        "\n",
        "g.add((ns.hasExternalInfo, RDF.type, RDF.Property))\n",
        "g.add((ns.hasExternalInfo, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasExternalInfo, RDFS.range, ns.ExternalDataSource))\n",
        "\n",
        "g.add((ns.hasIncomeGroup, RDF.type, RDF.Property))\n",
        "g.add((ns.hasIncomeGroup, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasIncomeGroup, RDFS.range, ns.IncomeGroup))\n",
        "\n",
        "g.add((ns.incomeFrom, RDF.type, RDF.Property))\n",
        "g.add((ns.incomeFrom, RDFS.domain, ns.IncomeGroup))\n",
        "g.add((ns.incomeFrom, RDFS.range, ns.IncomeSource))\n",
        "\n",
        "g.add((ns.paymentBehavior, RDF.type, RDF.Property))\n",
        "g.add((ns.paymentBehavior, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.paymentBehavior, RDFS.range, ns.Payment))\n",
        "\n",
        "# Define IRIs for Default and NotDefault classifications\n",
        "DEFAULT_IRI = ns.Default\n",
        "NOTDEFAULT_IRI = ns.NotDefault\n",
        "\n",
        "# Function to add properties to RDF node\n",
        "def add_properties(entity_uri, properties):\n",
        "    for prop, value in properties.items():\n",
        "        if pd.notna(value):\n",
        "            g.add((entity_uri, URIRef(ns + prop), value))  # Link as objects, not literals\n",
        "\n",
        "# Define income and age bands with ranges as band names\n",
        "income_bands = {\n",
        "    '0-50000': (0, 50000),\n",
        "    '50001-200000': (50001, 200000),\n",
        "    '200001-1000000': (200001, 1000000),\n",
        "    '> 1000000': (1000001, float('inf'))\n",
        "}\n",
        "\n",
        "age_bands = {\n",
        "    '18-30': (18, 30),\n",
        "    '31-50': (31, 50),\n",
        "    '51-70': (51, 70),\n",
        "    '> 71': (71, float('inf'))\n",
        "}\n",
        "\n",
        "def categorize_income(income):\n",
        "    for band, (low, high) in income_bands.items():\n",
        "        if low <= income <= high:\n",
        "            return band\n",
        "    return 'unknown'\n",
        "\n",
        "def categorize_age(age):\n",
        "    for band, (low, high) in age_bands.items():\n",
        "        if low <= age <= high:\n",
        "            return band\n",
        "    return 'unknown'\n",
        "\n",
        "# Process data in smaller batches\n",
        "batch_size = 1000  # Adjust batch size based on available memory\n",
        "\n",
        "# Dictionary to hold unique categorical value URIs\n",
        "unique_values_uris = {}\n",
        "\n",
        "# Function to get or create a URI for a unique categorical value\n",
        "def get_or_create_uri(category, value):\n",
        "    value = urllib.parse.quote(str(value).replace(' ', '_').replace('/', '_'))\n",
        "    if (category, value) not in unique_values_uris:\n",
        "        uri = URIRef(ns + f\"{category}_{value}\")\n",
        "        unique_values_uris[(category, value)] = uri\n",
        "        g.add((uri, RDF.type, ns[category]))\n",
        "        g.add((uri, RDFS.label, Literal(value)))\n",
        "    return unique_values_uris[(category, value)]\n",
        "\n",
        "for start in range(0, len(merged_df), batch_size):\n",
        "    end = min(start + batch_size, len(merged_df))\n",
        "    batch = merged_df.iloc[start:end]\n",
        "\n",
        "    for index, row in batch.iterrows():\n",
        "        applicant_uri = URIRef(ns + f\"Applicant_{row['SK_ID_CURR']}\")\n",
        "        g.add((applicant_uri, RDF.type, ns.Applicant))\n",
        "\n",
        "        # Categorize income and age\n",
        "        income_band = categorize_income(row['AMT_INCOME_TOTAL'])\n",
        "        age_band = categorize_age(row['DAYS_BIRTH'] // -365)  # Convert days to years\n",
        "\n",
        "        # Add Applicant properties\n",
        "        applicant_properties = {\n",
        "            \"hasLoanType\": get_or_create_uri('LoanType', row.get('NAME_CONTRACT_TYPE')),\n",
        "            \"hasGender\": get_or_create_uri('Gender', row.get('CODE_GENDER')),\n",
        "            \"ownsCar\": get_or_create_uri('OwnsCar', row.get('FLAG_OWN_CAR')),\n",
        "            \"ownsRealty\": get_or_create_uri('OwnsRealty', row.get('FLAG_OWN_REALTY')),\n",
        "            \"hasChildren\": Literal(row.get('CNT_CHILDREN'), datatype=XSD.integer),\n",
        "            \"hasIncomeGroup\": get_or_create_uri('IncomeGroup', income_band),\n",
        "            \"hasCreditAmount\": Literal(row.get('AMT_CREDIT'), datatype=XSD.decimal),\n",
        "            \"hasAnnuity\": Literal(row.get('AMT_ANNUITY'), datatype=XSD.decimal),\n",
        "            \"hasGoodsPrice\": Literal(row.get('AMT_GOODS_PRICE'), datatype=XSD.decimal),\n",
        "            \"hasIncomeType\": get_or_create_uri('IncomeType', row.get('NAME_INCOME_TYPE')),\n",
        "            \"hasEducationType\": get_or_create_uri('EducationType', row.get('NAME_EDUCATION_TYPE')),\n",
        "            \"hasFamilyStatus\": get_or_create_uri('FamilyStatus', row.get('NAME_FAMILY_STATUS')),\n",
        "            \"hasHousingType\": get_or_create_uri('HousingType', row.get('NAME_HOUSING_TYPE')),\n",
        "            \"hasRegionPopulationRelative\": Literal(row.get('REGION_POPULATION_RELATIVE'), datatype=XSD.decimal),\n",
        "            \"hasAgeGroup\": get_or_create_uri('AgeGroup', age_band),\n",
        "            \"hasDaysEmployed\": Literal(row.get('DAYS_EMPLOYED'), datatype=XSD.integer),\n",
        "            \"hasEmploymentStatus\": get_or_create_uri('EmploymentStatus', row.get('JOB'))\n",
        "        }\n",
        "        add_properties(applicant_uri, applicant_properties)\n",
        "\n",
        "        # Add Loan Default classification\n",
        "        if row['TARGET'] == 1:\n",
        "            g.add((applicant_uri, ns.classifiedAs, DEFAULT_IRI))\n",
        "        else:\n",
        "            g.add((applicant_uri, ns.classifiedAs, NOTDEFAULT_IRI))\n",
        "\n",
        "        # Add relationships\n",
        "        if 'SK_ID_BUREAU' in row and pd.notna(row['SK_ID_BUREAU']):\n",
        "            bureau_uri = URIRef(ns + f\"Bureau_{row['SK_ID_BUREAU']}\")\n",
        "            g.add((bureau_uri, RDF.type, ns.Bureau))\n",
        "            g.add((applicant_uri, ns.hasCreditHistory, bureau_uri))\n",
        "            bureau_properties = {\n",
        "                \"isActive\": get_or_create_uri('CreditActive', row.get('CREDIT_ACTIVE')),\n",
        "                \"hasCreditCurrency\": get_or_create_uri('CreditCurrency', row.get('CREDIT_CURRENCY')),\n",
        "                \"hasDaysCredit\": Literal(row.get('DAYS_CREDIT'), datatype=XSD.integer),\n",
        "                \"hasCreditDayOverdue\": Literal(row.get('CREDIT_DAY_OVERDUE'), datatype=XSD.integer),\n",
        "                \"hasDaysCreditEndDate\": Literal(row.get('DAYS_CREDIT_ENDDATE'), datatype=XSD.integer),\n",
        "                \"hasDaysEndDateFact\": Literal(row.get('DAYS_ENDDATE_FACT'), datatype=XSD.integer),\n",
        "                \"hasCreditMaxOverdue\": Literal(row.get('AMT_CREDIT_MAX_OVERDUE'), datatype=XSD.decimal),\n",
        "                \"hasCreditProlongCount\": Literal(row.get('CNT_CREDIT_PROLONG'), datatype=XSD.integer),\n",
        "                \"hasCreditSum\": Literal(row.get('AMT_CREDIT_SUM'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumDebt\": Literal(row.get('AMT_CREDIT_SUM_DEBT'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumLimit\": Literal(row.get('AMT_CREDIT_SUM_LIMIT'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumOverdue\": Literal(row.get('AMT_CREDIT_SUM_OVERDUE'), datatype=XSD.decimal),\n",
        "                \"hasCreditType\": get_or_create_uri('CreditType', row.get('CREDIT_TYPE')),\n",
        "                \"hasDaysCreditUpdate\": Literal(row.get('DAYS_CREDIT_UPDATE'), datatype=XSD.integer),\n",
        "                \"hasAnnuity\": Literal(row.get('AMT_ANNUITY'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(bureau_uri, bureau_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']):\n",
        "            prev_app_uri = URIRef(ns + f\"PreviousApplication_{row['SK_ID_PREV']}\")\n",
        "            g.add((prev_app_uri, RDF.type, ns.LoanApplication))\n",
        "            g.add((applicant_uri, ns.appliedPreviously, prev_app_uri))\n",
        "            prev_app_properties = {\n",
        "                \"hasContractType\": get_or_create_uri('ContractType', row.get('NAME_CONTRACT_TYPE_y')),\n",
        "                \"hasAnnuityAmount\": Literal(row.get('AMT_ANNUITY_y'), datatype=XSD.decimal),\n",
        "                \"hasApplicationAmount\": Literal(row.get('AMT_APPLICATION'), datatype=XSD.decimal),\n",
        "                \"hasCreditAmount\": Literal(row.get('AMT_CREDIT_y'), datatype=XSD.decimal),\n",
        "                \"hasGoodsPrice\": Literal(row.get('AMT_GOODS_PRICE_y'), datatype=XSD.decimal),\n",
        "                \"hasDecisionDays\": Literal(row.get('DAYS_DECISION'), datatype=XSD.integer),\n",
        "                \"hasPaymentType\": get_or_create_uri('PaymentType', row.get('NAME_PAYMENT_TYPE')),\n",
        "                \"hasClientType\": get_or_create_uri('ClientType', row.get('NAME_CLIENT_TYPE')),\n",
        "                \"hasProductType\": get_or_create_uri('ProductType', row.get('NAME_PRODUCT_TYPE')),\n",
        "                \"hasSellerIndustry\": get_or_create_uri('SellerIndustry', row.get('NAME_SELLER_INDUSTRY')),\n",
        "                \"hasDownPayment\": Literal(row.get('AMT_DOWN_PAYMENT'), datatype=XSD.decimal),\n",
        "                \"hasWeekdayApproval\": get_or_create_uri('WeekdayApproval', row.get('WEEKDAY_APPR_PROCESS_START')),\n",
        "                \"hasHourApproval\": Literal(row.get('HOUR_APPR_PROCESS_START'), datatype=XSD.integer)\n",
        "            }\n",
        "            add_properties(prev_app_uri, prev_app_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']) and 'AMT_BALANCE' in row and pd.notna(row['AMT_BALANCE']):\n",
        "            credit_card_uri = URIRef(ns + f\"CreditCard_{row['SK_ID_PREV']}\")\n",
        "            g.add((credit_card_uri, RDF.type, ns.CreditCard))\n",
        "            g.add((applicant_uri, ns.hasCreditCard, credit_card_uri))\n",
        "            credit_card_properties = {\n",
        "                \"hasBalanceAmount\": Literal(row.get('AMT_BALANCE'), datatype=XSD.decimal),\n",
        "                \"hasCreditLimitActual\": Literal(row.get('AMT_CREDIT_LIMIT_ACTUAL'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountATM\": Literal(row.get('AMT_DRAWINGS_ATM_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountCurrent\": Literal(row.get('AMT_DRAWINGS_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountOther\": Literal(row.get('AMT_DRAWINGS_OTHER_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountPOS\": Literal(row.get('AMT_DRAWINGS_POS_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasInstallmentRegularity\": Literal(row.get('AMT_INST_MIN_REGULARITY'), datatype=XSD.decimal),\n",
        "                \"hasPaymentCurrent\": Literal(row.get('AMT_PAYMENT_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasPaymentTotalCurrent\": Literal(row.get('AMT_PAYMENT_TOTAL_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasReceivablePrincipal\": Literal(row.get('AMT_RECEIVABLE_PRINCIPAL'), datatype=XSD.decimal),\n",
        "                \"hasReceivable\": Literal(row.get('AMT_RECIVABLE'), datatype=XSD.decimal),\n",
        "                \"hasTotalReceivable\": Literal(row.get('AMT_TOTAL_RECEIVABLE'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(credit_card_uri, credit_card_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']) and 'AMT_INSTALMENT' in row and pd.notna(row['AMT_INSTALMENT']):\n",
        "            installments_uri = URIRef(ns + f\"Installments_{row['SK_ID_PREV']}\")\n",
        "            g.add((installments_uri, RDF.type, ns.Payment))\n",
        "            g.add((applicant_uri, ns.paymentBehavior, installments_uri))\n",
        "            installments_properties = {\n",
        "                \"hasInstallmentVersion\": Literal(row.get('NUM_INSTALMENT_VERSION'), datatype=XSD.integer),\n",
        "                \"hasInstallmentNumber\": Literal(row.get('NUM_INSTALMENT_NUMBER'), datatype=XSD.integer),\n",
        "                \"hasDaysInstallment\": Literal(row.get('DAYS_INSTALMENT'), datatype=XSD.integer),\n",
        "                \"hasDaysEntryPayment\": Literal(row.get('DAYS_ENTRY_PAYMENT'), datatype=XSD.integer),\n",
        "                \"hasInstallmentAmount\": Literal(row.get('AMT_INSTALMENT'), datatype=XSD.decimal),\n",
        "                \"hasPaymentAmount\": Literal(row.get('AMT_PAYMENT'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(installments_uri, installments_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']) and 'CNT_INSTALMENT' in row and pd.notna(row['CNT_INSTALMENT']):\n",
        "            pos_cash_uri = URIRef(ns + f\"POSCash_{row['SK_ID_PREV']}\")\n",
        "            g.add((pos_cash_uri, RDF.type, ns.Payment))\n",
        "            g.add((applicant_uri, ns.paymentBehavior, pos_cash_uri))\n",
        "            pos_cash_properties = {\n",
        "                \"hasInstalmentCount\": Literal(row.get('CNT_INSTALMENT'), datatype=XSD.integer),\n",
        "                \"hasInstalmentFutureCount\": Literal(row.get('CNT_INSTALMENT_FUTURE'), datatype=XSD.integer),\n",
        "                \"hasContractStatus\": get_or_create_uri('ContractStatus', row.get('NAME_CONTRACT_STATUS_x')),\n",
        "                \"hasDaysPastDue\": Literal(row.get('SK_DPD'), datatype=XSD.integer),\n",
        "                \"hasDaysPastDueDef\": Literal(row.get('SK_DPD_DEF'), datatype=XSD.integer)\n",
        "            }\n",
        "            add_properties(pos_cash_uri, pos_cash_properties)\n",
        "\n",
        "# Save RDF graph to a file\n",
        "g.serialize(destination='loanKG_Final.rdf', format='turtle')\n",
        "\n",
        "print(\"RDF knowledge graph saved to 'loanKG_Final.rdf'\")\n"
      ],
      "metadata": {
        "id": "68PVt9VWrNqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dab948-75fa-413d-8cba-118cf2bf8d0a"
      },
      "id": "68PVt9VWrNqd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rdflib/term.py:1585: UserWarning: Serializing weird numerical rdflib.term.Literal('None', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#decimal'))\n",
            "  warnings.warn(\"Serializing weird numerical %r\" % self)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDF knowledge graph saved to 'loanKG_Final.rdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "from rdflib import Graph, Namespace, RDF, RDFS, URIRef, Literal, XSD\n",
        "\n",
        "# Read the CSV file\n",
        "#merged_df = pd.read_csv(\"/content/DataSet/merged_sampled_application_data_2.csv\")\n",
        "\n",
        "# Define namespaces\n",
        "ns = Namespace(\"http://example.org/ontology#\")\n",
        "RDF_NS = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "RDFS_NS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "\n",
        "# Create a new RDF graph\n",
        "g = Graph()\n",
        "g.bind(\"ex\", ns)\n",
        "g.bind(\"rdf\", RDF_NS)\n",
        "g.bind(\"rdfs\", RDFS_NS)\n",
        "\n",
        "# Define classes and properties from the T-Box schema\n",
        "classes = [\n",
        "    ns.Applicant, ns.AgeGroup, ns.Bureau, ns.CreditCard, ns.IncomeGroup, ns.IncomeSource,\n",
        "    ns.Loan, ns.LoanApplication, ns.Payment, ns.ExternalDataSource, ns.LoanStatus\n",
        "]\n",
        "\n",
        "subclasses = [ns.Default, ns.NotDefault]\n",
        "\n",
        "properties = [\n",
        "    (ns.appliedFor, ns.LoanApplication, ns.Loan),\n",
        "    (ns.appliedPreviously, ns.Applicant, ns.LoanApplication),\n",
        "    (ns.classifiedAs, ns.Applicant, ns.LoanStatus),\n",
        "    (ns.hasAgeGroup, ns.Applicant, ns.AgeGroup),\n",
        "    (ns.hasCreditCard, ns.Applicant, ns.CreditCard),\n",
        "    (ns.hasCreditHistory, ns.Applicant, ns.Bureau),\n",
        "    (ns.hasExistingLoan, ns.Applicant, ns.Loan),\n",
        "    (ns.hasExternalInfo, ns.Applicant, ns.ExternalDataSource),\n",
        "    (ns.hasIncomeGroup, ns.Applicant, ns.IncomeGroup),\n",
        "    (ns.incomeFrom, ns.IncomeGroup, ns.IncomeSource),\n",
        "    (ns.paymentBehavior, ns.Applicant, ns.Payment)\n",
        "]\n",
        "\n",
        "# Add classes to graph\n",
        "for cls in classes:\n",
        "    g.add((cls, RDF.type, RDFS.Class))\n",
        "\n",
        "# Add subclasses to graph\n",
        "for subclass in subclasses:\n",
        "    g.add((subclass, RDF.type, ns.LoanStatus))\n",
        "\n",
        "# Add properties to graph\n",
        "for prop, domain, range_ in properties:\n",
        "    g.add((prop, RDF.type, RDF.Property))\n",
        "    g.add((prop, RDFS.domain, domain))\n",
        "    g.add((prop, RDFS.range, range_))\n",
        "\n",
        "# Define IRIs for Default and NotDefault classifications\n",
        "DEFAULT_IRI = ns.Default\n",
        "NOTDEFAULT_IRI = ns.NotDefault\n",
        "\n",
        "# Function to add properties to RDF node\n",
        "def add_properties(entity_uri, properties):\n",
        "    for prop, value in properties.items():\n",
        "        if pd.notna(value):\n",
        "            g.add((entity_uri, URIRef(ns + prop), value))  # Link as objects, not literals\n",
        "\n",
        "# Define income and age bands with ranges as band names\n",
        "income_bands = {\n",
        "    '0-50000': (0, 50000),\n",
        "    '50001-200000': (50001, 200000),\n",
        "    '200001-1000000': (200001, 1000000),\n",
        "    '> 1000000': (1000001, float('inf'))\n",
        "}\n",
        "\n",
        "age_bands = {\n",
        "    '18-30': (18, 30),\n",
        "    '31-50': (31, 50),\n",
        "    '51-70': (51, 70),\n",
        "    '> 71': (71, float('inf'))\n",
        "}\n",
        "\n",
        "def categorize_income(income):\n",
        "    for band, (low, high) in income_bands.items():\n",
        "        if low <= income <= high:\n",
        "            return band\n",
        "    return 'unknown'\n",
        "\n",
        "def categorize_age(age):\n",
        "    for band, (low, high) in age_bands.items():\n",
        "        if low <= age <= high:\n",
        "            return band\n",
        "    return 'unknown'\n",
        "\n",
        "# Process data in smaller batches\n",
        "batch_size = 1000  # Adjust batch size based on available memory\n",
        "\n",
        "# Dictionary to hold unique categorical value URIs\n",
        "unique_values_uris = {}\n",
        "\n",
        "# Function to get or create a URI for a unique categorical value\n",
        "def get_or_create_uri(category, value):\n",
        "    value = urllib.parse.quote(str(value).replace(' ', '_').replace('/', '_'))\n",
        "    if (category, value) not in unique_values_uris:\n",
        "        uri = URIRef(ns + f\"{category}_{value}\")\n",
        "        unique_values_uris[(category, value)] = uri\n",
        "        g.add((uri, RDF.type, ns[category]))\n",
        "        g.add((uri, RDFS.label, Literal(value)))\n",
        "    return unique_values_uris[(category, value)]\n",
        "\n",
        "for start in range(0, len(merged_df), batch_size):\n",
        "    end = min(start + batch_size, len(merged_df))\n",
        "    batch = merged_df.iloc[start:end]\n",
        "\n",
        "    for index, row in batch.iterrows():\n",
        "        applicant_uri = URIRef(ns + f\"Applicant_{row['SK_ID_CURR']}\")\n",
        "        g.add((applicant_uri, RDF.type, ns.Applicant))\n",
        "\n",
        "        # Categorize income and age\n",
        "        income_band = categorize_income(row['AMT_INCOME_TOTAL'])\n",
        "        age_band = categorize_age(row['DAYS_BIRTH'] // -365)  # Convert days to years\n",
        "\n",
        "        # Add Applicant properties\n",
        "        applicant_properties = {\n",
        "            \"hasLoanType\": get_or_create_uri('LoanType', row.get('NAME_CONTRACT_TYPE')),\n",
        "            \"hasGender\": get_or_create_uri('Gender', row.get('CODE_GENDER')),\n",
        "            \"ownsCar\": get_or_create_uri('OwnsCar', row.get('FLAG_OWN_CAR')),\n",
        "            \"ownsRealty\": get_or_create_uri('OwnsRealty', row.get('FLAG_OWN_REALTY')),\n",
        "            \"hasChildren\": Literal(row.get('CNT_CHILDREN'), datatype=XSD.integer),\n",
        "            \"hasIncomeGroup\": get_or_create_uri('IncomeGroup', income_band),\n",
        "            \"hasCreditAmount\": Literal(row.get('AMT_CREDIT'), datatype=XSD.decimal),\n",
        "            \"hasAnnuity\": Literal(row.get('AMT_ANNUITY'), datatype=XSD.decimal),\n",
        "            \"hasGoodsPrice\": Literal(row.get('AMT_GOODS_PRICE'), datatype=XSD.decimal),\n",
        "            \"hasIncomeType\": get_or_create_uri('IncomeType', row.get('NAME_INCOME_TYPE')),\n",
        "            \"hasEducationType\": get_or_create_uri('EducationType', row.get('NAME_EDUCATION_TYPE')),\n",
        "            \"hasFamilyStatus\": get_or_create_uri('FamilyStatus', row.get('NAME_FAMILY_STATUS')),\n",
        "            \"hasHousingType\": get_or_create_uri('HousingType', row.get('NAME_HOUSING_TYPE')),\n",
        "            \"hasRegionPopulationRelative\": Literal(row.get('REGION_POPULATION_RELATIVE'), datatype=XSD.decimal),\n",
        "            \"hasAgeGroup\": get_or_create_uri('AgeGroup', age_band),\n",
        "            \"hasDaysEmployed\": Literal(row.get('DAYS_EMPLOYED'), datatype=XSD.integer),\n",
        "            \"hasEmploymentStatus\": get_or_create_uri('EmploymentStatus', row.get('JOB'))\n",
        "        }\n",
        "        add_properties(applicant_uri, applicant_properties)\n",
        "\n",
        "        # Add Loan Default classification\n",
        "        if row['TARGET'] == 1:\n",
        "            g.add((applicant_uri, ns.classifiedAs, DEFAULT_IRI))\n",
        "        else:\n",
        "            g.add((applicant_uri, ns.classifiedAs, NOTDEFAULT_IRI))\n",
        "\n",
        "        # Add relationships\n",
        "        if 'SK_ID_BUREAU' in row and pd.notna(row['SK_ID_BUREAU']):\n",
        "            bureau_uri = URIRef(ns + f\"Bureau_{row['SK_ID_BUREAU']}\")\n",
        "            g.add((bureau_uri, RDF.type, ns.Bureau))\n",
        "            g.add((applicant_uri, ns.hasCreditHistory, bureau_uri))\n",
        "            bureau_properties = {\n",
        "                \"isActive\": get_or_create_uri('CreditActive', row.get('CREDIT_ACTIVE')),\n",
        "                \"hasCreditCurrency\": get_or_create_uri('CreditCurrency', row.get('CREDIT_CURRENCY')),\n",
        "                \"hasDaysCredit\": Literal(row.get('DAYS_CREDIT'), datatype=XSD.integer),\n",
        "                \"hasCreditDayOverdue\": Literal(row.get('CREDIT_DAY_OVERDUE'), datatype=XSD.integer),\n",
        "                \"hasDaysCreditEndDate\": Literal(row.get('DAYS_CREDIT_ENDDATE'), datatype=XSD.integer),\n",
        "                \"hasDaysEndDateFact\": Literal(row.get('DAYS_ENDDATE_FACT'), datatype=XSD.integer),\n",
        "                \"hasCreditMaxOverdue\": Literal(row.get('AMT_CREDIT_MAX_OVERDUE'), datatype=XSD.decimal),\n",
        "                \"hasCreditProlongCount\": Literal(row.get('CNT_CREDIT_PROLONG'), datatype=XSD.integer),\n",
        "                \"hasCreditSum\": Literal(row.get('AMT_CREDIT_SUM'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumDebt\": Literal(row.get('AMT_CREDIT_SUM_DEBT'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumLimit\": Literal(row.get('AMT_CREDIT_SUM_LIMIT'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumOverdue\": Literal(row.get('AMT_CREDIT_SUM_OVERDUE'), datatype=XSD.decimal),\n",
        "                \"hasCreditType\": get_or_create_uri('CreditType', row.get('CREDIT_TYPE')),\n",
        "                \"hasDaysCreditUpdate\": Literal(row.get('DAYS_CREDIT_UPDATE'), datatype=XSD.integer),\n",
        "                \"hasAnnuity\": Literal(row.get('AMT_ANNUITY'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(bureau_uri, bureau_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']):\n",
        "            prev_app_uri = URIRef(ns + f\"PreviousApplication_{row['SK_ID_PREV']}\")\n",
        "            g.add((prev_app_uri, RDF.type, ns.LoanApplication))\n",
        "            g.add((applicant_uri, ns.appliedPreviously, prev_app_uri))\n",
        "\n",
        "        if 'SK_ID_CURR' in row and pd.notna(row['SK_ID_CURR']):\n",
        "            curr_app_uri = URIRef(ns + f\"CurrentApplication_{row['SK_ID_CURR']}\")\n",
        "            g.add((curr_app_uri, RDF.type, ns.LoanApplication))\n",
        "            g.add((applicant_uri, ns.appliedFor, curr_app_uri))\n",
        "\n",
        "# Save the RDF graph in Turtle format\n",
        "output_file = \"/content/DataSet/output_graph.ttl\"\n",
        "g.serialize(destination=output_file, format=\"turtle\")\n",
        "\n",
        "print(f\"RDF graph has been saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "p2elzmxacVwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fffef1-5a87-4c16-8f69-1bf9947d2ce5"
      },
      "id": "p2elzmxacVwN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDF graph has been saved to /content/DataSet/output_graph.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# specify file paths\n",
        "source_path = '/content/DataSet/output_graph.ttl'\n",
        "destination_path = '/content/drive/MyDrive/loanKG_Final.ttl'\n",
        "\n",
        "# use shutil to copy the file\n",
        "shutil.copy(source_path, destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bpp6ZYfb7RRE",
        "outputId": "db611fdb-41a1-4694-9670-260ed9051936"
      },
      "id": "bpp6ZYfb7RRE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/loanKG_Final.ttl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TjSQFo30o4cP"
      },
      "id": "TjSQFo30o4cP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahmad Rajeh code\n",
        "columns are assumed to be categorical and are converted to nodes in the RDF graph:\n",
        "\n",
        "NAME_CONTRACT_TYPE (LoanType)\n",
        "CODE_GENDER (Gender)\n",
        "FLAG_OWN_CAR (OwnsCar)\n",
        "FLAG_OWN_REALTY (OwnsRealty)\n",
        "NAME_INCOME_TYPE (IncomeType)\n",
        "NAME_EDUCATION_TYPE (EducationType)\n",
        "NAME_FAMILY_STATUS (FamilyStatus)\n",
        "NAME_HOUSING_TYPE (HousingType)\n",
        "OCCUPATION_TYPE (OccupationType)\n",
        "CREDIT_ACTIVE (CreditActive)\n",
        "CREDIT_CURRENCY (CreditCurrency)\n",
        "CREDIT_TYPE (CreditType)\n",
        "NAME_PAYMENT_TYPE (PaymentType)\n",
        "NAME_CLIENT_TYPE (ClientType)\n",
        "NAME_PRODUCT_TYPE (ProductType)\n",
        "NAME_SELLER_INDUSTRY (SellerIndustry)\n",
        "WEEKDAY_APPR_PROCESS_START (WeekdayApproval)\n",
        "NAME_CONTRACT_STATUS_x (ContractStatus)\n",
        "Additionally, income and age are grouped into bands, which are also treated as categorical for the purpose of creating nodes:\n",
        "\n",
        "Income bands (AMT_INCOME_TOTAL) grouped as:\n",
        "\n",
        "0-50000\n",
        "50001-200000\n",
        "200001-1000000\n",
        "> 1000000\n",
        "Age bands (DAYS_BIRTH) grouped as:\n",
        "\n",
        "18-30\n",
        "31-50\n",
        "51-70\n",
        "> 71\n",
        "These columns are converted to URIs (nodes) in the RDF graph to represent their unique values as distinct entities. This approach avoids repeating literals and links occurrences to these unique nodes."
      ],
      "metadata": {
        "id": "OSyq7zKTOSLI"
      },
      "id": "OSyq7zKTOSLI"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdflib import Graph, Namespace, URIRef, Literal\n",
        "from rdflib.namespace import RDF, RDFS, XSD\n",
        "import urllib.parse\n",
        "\n",
        "# Adjust sampling fraction as needed\n",
        "sampling_fraction = 0.01\n",
        "\n",
        "# Load and sample data files\n",
        "application_train = pd.read_csv('/content/drive/MyDrive/DataSet/application_train.csv')\n",
        "application_train = application_train.dropna(subset=['SK_ID_CURR', 'TARGET', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS'])\n",
        "application_train = application_train.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "bureau = pd.read_csv('/content/drive/MyDrive/DataSet/bureau.csv')\n",
        "bureau = bureau.dropna(subset=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
        "bureau = bureau.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "bureau_balance = pd.read_csv('/content/drive/MyDrive/DataSet/bureau_balance.csv')\n",
        "bureau_balance = bureau_balance.dropna(subset=['SK_ID_BUREAU'])\n",
        "bureau_balance = bureau_balance.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "credit_card_balance = pd.read_csv('/content/drive/MyDrive/DataSet/credit_card_balance.csv')\n",
        "credit_card_balance = credit_card_balance.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "credit_card_balance = credit_card_balance.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "installments_payments = pd.read_csv('/content/drive/MyDrive/DataSet/installments_payments.csv')\n",
        "installments_payments = installments_payments.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "installments_payments = installments_payments.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "pos_cash_balance = pd.read_csv('/content/drive/MyDrive/DataSet/POS_CASH_balance.csv')\n",
        "pos_cash_balance = pos_cash_balance.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "pos_cash_balance = pos_cash_balance.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "previous_application = pd.read_csv('/content/drive/MyDrive/DataSet/previous_application.csv')\n",
        "previous_application = previous_application.dropna(subset=['SK_ID_CURR', 'SK_ID_PREV'])\n",
        "previous_application = previous_application.sample(frac=sampling_fraction, random_state=42)\n",
        "\n",
        "# Merge dataframes with specified suffixes to avoid column name conflicts\n",
        "merged_df = application_train.copy()\n",
        "merged_df = merged_df.merge(bureau, on='SK_ID_CURR', how='left', suffixes=('', '_bureau'))\n",
        "merged_df = merged_df.merge(bureau_balance, on='SK_ID_BUREAU', how='left', suffixes=('', '_bureau_balance'))\n",
        "merged_df = merged_df.merge(credit_card_balance, on='SK_ID_CURR', how='left', suffixes=('', '_credit_card'))\n",
        "merged_df = merged_df.merge(installments_payments, on='SK_ID_CURR', how='left', suffixes=('', '_installments'))\n",
        "merged_df = merged_df.merge(pos_cash_balance, on='SK_ID_CURR', how='left', suffixes=('', '_pos_cash'))\n",
        "merged_df = merged_df.merge(previous_application, on='SK_ID_CURR', how='left', suffixes=('', '_previous'))\n",
        "\n",
        "merged_df.to_csv('/content/drive/MyDrive/DataSet/merged_sampled_application_data_2.csv', index=False)\n",
        "\n",
        "# Define namespaces\n",
        "ns = Namespace(\"http://example.org/ontology#\")\n",
        "RDF_NS = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "RDFS_NS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "\n",
        "# Create a new RDF graph\n",
        "g = Graph()\n",
        "g.bind(\"ex\", ns)\n",
        "g.bind(\"rdf\", RDF_NS)\n",
        "g.bind(\"rdfs\", RDFS_NS)\n",
        "\n",
        "# Define classes and properties from the T-Box schema\n",
        "g.add((ns.Applicant, RDF.type, RDFS.Class))\n",
        "g.add((ns.AgeGroup, RDF.type, RDFS.Class))\n",
        "g.add((ns.Bureau, RDF.type, RDFS.Class))\n",
        "g.add((ns.CreditCard, RDF.type, RDFS.Class))\n",
        "g.add((ns.Default, RDF.type, RDFS.Class))\n",
        "g.add((ns.IncomeGroup, RDF.type, RDFS.Class))\n",
        "g.add((ns.IncomeSource, RDF.type, RDFS.Class))\n",
        "g.add((ns.Loan, RDF.type, RDFS.Class))\n",
        "g.add((ns.LoanApplication, RDF.type, RDFS.Class))\n",
        "g.add((ns.Payment, RDF.type, RDFS.Class))\n",
        "g.add((ns.ExternalDataSource, RDF.type, RDFS.Class))\n",
        "\n",
        "g.add((ns.appliedFor, RDF.type, RDF.Property))\n",
        "g.add((ns.appliedFor, RDFS.domain, ns.LoanApplication))\n",
        "g.add((ns.appliedFor, RDFS.range, ns.Loan))\n",
        "\n",
        "g.add((ns.appliedPreviously, RDF.type, RDF.Property))\n",
        "g.add((ns.appliedPreviously, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.appliedPreviously, RDFS.range, ns.LoanApplication))\n",
        "\n",
        "g.add((ns.classifiedAs, RDF.type, RDF.Property))\n",
        "g.add((ns.classifiedAs, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.classifiedAs, RDFS.range, ns.Default))\n",
        "\n",
        "g.add((ns.hasAgeGroup, RDF.type, RDF.Property))\n",
        "g.add((ns.hasAgeGroup, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasAgeGroup, RDFS.range, ns.AgeGroup))\n",
        "\n",
        "g.add((ns.hasCreditCard, RDF.type, RDF.Property))\n",
        "g.add((ns.hasCreditCard, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasCreditCard, RDFS.range, ns.CreditCard))\n",
        "\n",
        "g.add((ns.hasCreditHistory, RDF.type, RDF.Property))\n",
        "g.add((ns.hasCreditHistory, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasCreditHistory, RDFS.range, ns.Bureau))\n",
        "\n",
        "g.add((ns.hasExistingLoan, RDF.type, RDF.Property))\n",
        "g.add((ns.hasExistingLoan, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasExistingLoan, RDFS.range, ns.Loan))\n",
        "\n",
        "g.add((ns.hasExternalInfo, RDF.type, RDF.Property))\n",
        "g.add((ns.hasExternalInfo, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasExternalInfo, RDFS.range, ns.ExternalDataSource))\n",
        "\n",
        "g.add((ns.hasIncomeGroup, RDF.type, RDF.Property))\n",
        "g.add((ns.hasIncomeGroup, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.hasIncomeGroup, RDFS.range, ns.IncomeGroup))\n",
        "\n",
        "g.add((ns.incomeFrom, RDF.type, RDF.Property))\n",
        "g.add((ns.incomeFrom, RDFS.domain, ns.IncomeGroup))\n",
        "g.add((ns.incomeFrom, RDFS.range, ns.IncomeSource))\n",
        "\n",
        "g.add((ns.paymentBehavior, RDF.type, RDF.Property))\n",
        "g.add((ns.paymentBehavior, RDFS.domain, ns.Applicant))\n",
        "g.add((ns.paymentBehavior, RDFS.range, ns.Payment))\n",
        "\n",
        "# Function to add properties to RDF node\n",
        "def add_properties(entity_uri, properties):\n",
        "    for prop, value in properties.items():\n",
        "        if pd.notna(value):\n",
        "            g.add((entity_uri, URIRef(ns + prop), value))  # Link as objects, not literals\n",
        "\n",
        "# Define income and age bands with ranges as band names\n",
        "income_bands = {\n",
        "    '0-50000': (0, 50000),\n",
        "    '50001-200000': (50001, 200000),\n",
        "    '200001-1000000': (200001, 1000000),\n",
        "    '> 1000000': (1000001, float('inf'))\n",
        "}\n",
        "\n",
        "age_bands = {\n",
        "    '18-30': (18, 30),\n",
        "    '31-50': (31, 50),\n",
        "    '51-70': (51, 70),\n",
        "    '> 71': (71, float('inf'))\n",
        "}\n",
        "\n",
        "def categorize_income(income):\n",
        "    for band, (low, high) in income_bands.items():\n",
        "        if low <= income <= high:\n",
        "            return band\n",
        "    return 'unknown'\n",
        "\n",
        "def categorize_age(age):\n",
        "    for band, (low, high) in age_bands.items():\n",
        "        if low <= age <= high:\n",
        "            return band\n",
        "    return 'unknown'\n",
        "\n",
        "# Process data in smaller batches\n",
        "batch_size = 1000  # Adjust batch size based on available memory\n",
        "\n",
        "# Dictionary to hold unique categorical value URIs\n",
        "unique_values_uris = {}\n",
        "\n",
        "# Function to get or create a URI for a unique categorical value\n",
        "def get_or_create_uri(category, value):\n",
        "    value = urllib.parse.quote(str(value).replace(' ', '_').replace('/', '_'))\n",
        "    if (category, value) not in unique_values_uris:\n",
        "        uri = URIRef(ns + f\"{category}_{value}\")\n",
        "        unique_values_uris[(category, value)] = uri\n",
        "        g.add((uri, RDF.type, ns[category]))\n",
        "        g.add((uri, RDFS.label, Literal(value)))\n",
        "    return unique_values_uris[(category, value)]\n",
        "\n",
        "for start in range(0, len(merged_df), batch_size):\n",
        "    end = min(start + batch_size, len(merged_df))\n",
        "    batch = merged_df.iloc[start:end]\n",
        "\n",
        "    for index, row in batch.iterrows():\n",
        "        applicant_uri = URIRef(ns + f\"Applicant_{row['SK_ID_CURR']}\")\n",
        "        g.add((applicant_uri, RDF.type, ns.Applicant))\n",
        "\n",
        "        bureauIRI = URIRef(ns + f\"Bureau_{row['SK_ID_BUREAU']}\")\n",
        "\n",
        "        # Categorize income and age\n",
        "        income_band = categorize_income(row['AMT_INCOME_TOTAL'])\n",
        "        age_band = categorize_age(row['DAYS_BIRTH'] // -365)  # Convert days to years\n",
        "\n",
        "        # Add Applicant properties\n",
        "        applicant_properties = {\n",
        "            \"hasLoanType\": get_or_create_uri('LoanType', row.get('NAME_CONTRACT_TYPE')),\n",
        "            \"hasGender\": get_or_create_uri('Gender', row.get('CODE_GENDER')),\n",
        "            \"ownsCar\": get_or_create_uri('OwnsCar', row.get('FLAG_OWN_CAR')),\n",
        "            \"ownsRealty\": get_or_create_uri('OwnsRealty', row.get('FLAG_OWN_REALTY')),\n",
        "            \"hasChildren\": Literal(row.get('CNT_CHILDREN'), datatype=XSD.integer),\n",
        "            \"hasIncomeGroup\": get_or_create_uri('IncomeGroup', income_band),\n",
        "            \"hasCreditAmount\": Literal(row.get('AMT_CREDIT'), datatype=XSD.decimal),\n",
        "            \"hasAnnuity\": Literal(row.get('AMT_ANNUITY'), datatype=XSD.decimal),\n",
        "            \"hasGoodsPrice\": Literal(row.get('AMT_GOODS_PRICE'), datatype=XSD.decimal),\n",
        "            \"hasIncomeType\": get_or_create_uri('IncomeType', row.get('NAME_INCOME_TYPE')),\n",
        "            \"hasEducationType\": get_or_create_uri('EducationType', row.get('NAME_EDUCATION_TYPE')),\n",
        "            \"hasFamilyStatus\": get_or_create_uri('FamilyStatus', row.get('NAME_FAMILY_STATUS')),\n",
        "            \"hasHousingType\": get_or_create_uri('HousingType', row.get('NAME_HOUSING_TYPE')),\n",
        "            \"hasRegionPopulationRelative\": Literal(row.get('REGION_POPULATION_RELATIVE'), datatype=XSD.decimal),\n",
        "            \"hasAgeGroup\": get_or_create_uri('AgeGroup', age_band),\n",
        "            \"hasDaysEmployed\": Literal(row.get('DAYS_EMPLOYED'), datatype=XSD.integer),\n",
        "            \"hasOwnCarAge\": Literal(row.get('OWN_CAR_AGE'), datatype=XSD.integer),\n",
        "            \"hasOccupationType\": get_or_create_uri('OccupationType', row.get('OCCUPATION_TYPE')),\n",
        "            \"hasFamilyMembersCount\": Literal(row.get('CNT_FAM_MEMBERS'), datatype=XSD.integer),\n",
        "            \"hasBureauIRI\": bureauIRI\n",
        "        }\n",
        "        add_properties(applicant_uri, applicant_properties)\n",
        "\n",
        "        # Add Loan Default classification\n",
        "        g.add((applicant_uri, ns.classifiedAs, Literal(row['TARGET'], datatype=XSD.boolean)))\n",
        "\n",
        "        # Add relationships\n",
        "        if 'SK_ID_BUREAU' in row and pd.notna(row['SK_ID_BUREAU']):\n",
        "            bureau_uri = URIRef(ns + f\"Bureau_{row['SK_ID_BUREAU']}\")\n",
        "            g.add((bureau_uri, RDF.type, ns.Bureau))\n",
        "            g.add((applicant_uri, ns.hasCreditHistory, bureau_uri))\n",
        "            bureau_properties = {\n",
        "                \"isActive\": get_or_create_uri('CreditActive', row.get('CREDIT_ACTIVE')),\n",
        "                \"hasCreditCurrency\": get_or_create_uri('CreditCurrency', row.get('CREDIT_CURRENCY')),\n",
        "                \"hasDaysCredit\": Literal(row.get('DAYS_CREDIT'), datatype=XSD.integer),\n",
        "                \"hasCreditDayOverdue\": Literal(row.get('CREDIT_DAY_OVERDUE'), datatype=XSD.integer),\n",
        "                \"hasDaysCreditEndDate\": Literal(row.get('DAYS_CREDIT_ENDDATE'), datatype=XSD.integer),\n",
        "                \"hasDaysEndDateFact\": Literal(row.get('DAYS_ENDDATE_FACT'), datatype=XSD.integer),\n",
        "                \"hasCreditMaxOverdue\": Literal(row.get('AMT_CREDIT_MAX_OVERDUE'), datatype=XSD.decimal),\n",
        "                \"hasCreditProlongCount\": Literal(row.get('CNT_CREDIT_PROLONG'), datatype=XSD.integer),\n",
        "                \"hasCreditSum\": Literal(row.get('AMT_CREDIT_SUM'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumDebt\": Literal(row.get('AMT_CREDIT_SUM_DEBT'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumLimit\": Literal(row.get('AMT_CREDIT_SUM_LIMIT'), datatype=XSD.decimal),\n",
        "                \"hasCreditSumOverdue\": Literal(row.get('AMT_CREDIT_SUM_OVERDUE'), datatype=XSD.decimal),\n",
        "                \"hasCreditType\": get_or_create_uri('CreditType', row.get('CREDIT_TYPE')),\n",
        "                \"hasDaysCreditUpdate\": Literal(row.get('DAYS_CREDIT_UPDATE'), datatype=XSD.integer),\n",
        "                \"hasAnnuity\": Literal(row.get('AMT_ANNUITY'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(bureau_uri, bureau_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']):\n",
        "            prev_app_uri = URIRef(ns + f\"PreviousApplication_{row['SK_ID_PREV']}\")\n",
        "            g.add((prev_app_uri, RDF.type, ns.LoanApplication))\n",
        "            g.add((applicant_uri, ns.appliedPreviously, prev_app_uri))\n",
        "            prev_app_properties = {\n",
        "                \"hasContractType\": get_or_create_uri('ContractType', row.get('NAME_CONTRACT_TYPE_y')),\n",
        "                \"hasAnnuityAmount\": Literal(row.get('AMT_ANNUITY_y'), datatype=XSD.decimal),\n",
        "                \"hasApplicationAmount\": Literal(row.get('AMT_APPLICATION'), datatype=XSD.decimal),\n",
        "                \"hasCreditAmount\": Literal(row.get('AMT_CREDIT_y'), datatype=XSD.decimal),\n",
        "                \"hasGoodsPrice\": Literal(row.get('AMT_GOODS_PRICE_y'), datatype=XSD.decimal),\n",
        "                \"hasDecisionDays\": Literal(row.get('DAYS_DECISION'), datatype=XSD.integer),\n",
        "                \"hasPaymentType\": get_or_create_uri('PaymentType', row.get('NAME_PAYMENT_TYPE')),\n",
        "                \"hasClientType\": get_or_create_uri('ClientType', row.get('NAME_CLIENT_TYPE')),\n",
        "                \"hasProductType\": get_or_create_uri('ProductType', row.get('NAME_PRODUCT_TYPE')),\n",
        "                \"hasSellerIndustry\": get_or_create_uri('SellerIndustry', row.get('NAME_SELLER_INDUSTRY')),\n",
        "                \"hasDownPayment\": Literal(row.get('AMT_DOWN_PAYMENT'), datatype=XSD.decimal),\n",
        "                \"hasWeekdayApproval\": get_or_create_uri('WeekdayApproval', row.get('WEEKDAY_APPR_PROCESS_START')),\n",
        "                \"hasHourApproval\": Literal(row.get('HOUR_APPR_PROCESS_START'), datatype=XSD.integer)\n",
        "            }\n",
        "            add_properties(prev_app_uri, prev_app_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']) and 'AMT_BALANCE' in row and pd.notna(row['AMT_BALANCE']):\n",
        "            credit_card_uri = URIRef(ns + f\"CreditCard_{row['SK_ID_PREV']}\")\n",
        "            g.add((credit_card_uri, RDF.type, ns.CreditCard))\n",
        "            g.add((applicant_uri, ns.hasCreditCard, credit_card_uri))\n",
        "            credit_card_properties = {\n",
        "                \"hasBalanceAmount\": Literal(row.get('AMT_BALANCE'), datatype=XSD.decimal),\n",
        "                \"hasCreditLimitActual\": Literal(row.get('AMT_CREDIT_LIMIT_ACTUAL'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountATM\": Literal(row.get('AMT_DRAWINGS_ATM_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountCurrent\": Literal(row.get('AMT_DRAWINGS_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountOther\": Literal(row.get('AMT_DRAWINGS_OTHER_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasDrawingAmountPOS\": Literal(row.get('AMT_DRAWINGS_POS_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasInstallmentRegularity\": Literal(row.get('AMT_INST_MIN_REGULARITY'), datatype=XSD.decimal),\n",
        "                \"hasPaymentCurrent\": Literal(row.get('AMT_PAYMENT_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasPaymentTotalCurrent\": Literal(row.get('AMT_PAYMENT_TOTAL_CURRENT'), datatype=XSD.decimal),\n",
        "                \"hasReceivablePrincipal\": Literal(row.get('AMT_RECEIVABLE_PRINCIPAL'), datatype=XSD.decimal),\n",
        "                \"hasReceivable\": Literal(row.get('AMT_RECIVABLE'), datatype=XSD.decimal),\n",
        "                \"hasTotalReceivable\": Literal(row.get('AMT_TOTAL_RECEIVABLE'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(credit_card_uri, credit_card_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']) and 'AMT_INSTALMENT' in row and pd.notna(row['AMT_INSTALMENT']):\n",
        "            installments_uri = URIRef(ns + f\"Installments_{row['SK_ID_PREV']}\")\n",
        "            g.add((installments_uri, RDF.type, ns.Payment))\n",
        "            g.add((applicant_uri, ns.paymentBehavior, installments_uri))\n",
        "            installments_properties = {\n",
        "                \"hasInstallmentVersion\": Literal(row.get('NUM_INSTALMENT_VERSION'), datatype=XSD.integer),\n",
        "                \"hasInstallmentNumber\": Literal(row.get('NUM_INSTALMENT_NUMBER'), datatype=XSD.integer),\n",
        "                \"hasDaysInstallment\": Literal(row.get('DAYS_INSTALMENT'), datatype=XSD.integer),\n",
        "                \"hasDaysEntryPayment\": Literal(row.get('DAYS_ENTRY_PAYMENT'), datatype=XSD.integer),\n",
        "                \"hasInstallmentAmount\": Literal(row.get('AMT_INSTALMENT'), datatype=XSD.decimal),\n",
        "                \"hasPaymentAmount\": Literal(row.get('AMT_PAYMENT'), datatype=XSD.decimal)\n",
        "            }\n",
        "            add_properties(installments_uri, installments_properties)\n",
        "\n",
        "        if 'SK_ID_PREV' in row and pd.notna(row['SK_ID_PREV']) and 'CNT_INSTALMENT' in row and pd.notna(row['CNT_INSTALMENT']):\n",
        "            pos_cash_uri = URIRef(ns + f\"POSCash_{row['SK_ID_PREV']}\")\n",
        "            g.add((pos_cash_uri, RDF.type, ns.Payment))\n",
        "            g.add((applicant_uri, ns.paymentBehavior, pos_cash_uri))\n",
        "            pos_cash_properties = {\n",
        "                \"hasInstalmentCount\": Literal(row.get('CNT_INSTALMENT'), datatype=XSD.integer),\n",
        "                \"hasInstalmentFutureCount\": Literal(row.get('CNT_INSTALMENT_FUTURE'), datatype=XSD.integer),\n",
        "                \"hasContractStatus\": get_or_create_uri('ContractStatus', row.get('NAME_CONTRACT_STATUS_x')),\n",
        "                \"hasDaysPastDue\": Literal(row.get('SK_DPD'), datatype=XSD.integer),\n",
        "                \"hasDaysPastDueDef\": Literal(row.get('SK_DPD_DEF'), datatype=XSD.integer)\n",
        "            }\n",
        "            add_properties(pos_cash_uri, pos_cash_properties)\n",
        "\n",
        "# Save RDF graph to a file\n",
        "g.serialize(destination='loan_knowledge_graph_3.rdf', format='turtle')\n",
        "\n",
        "print(\"RDF knowledge graph saved to 'loan_knowledge_graph_3.rdf'\")\n"
      ],
      "metadata": {
        "id": "IdVLdlqDOR88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f03e0620-4ed5-4caf-c556-4d36466b42de"
      },
      "id": "IdVLdlqDOR88",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DataSet/application_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-21432d8c919a>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load and sample data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mapplication_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/DataSet/application_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mapplication_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplication_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SK_ID_CURR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TARGET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CODE_GENDER'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_OWN_CAR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_OWN_REALTY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNT_CHILDREN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_INCOME_TOTAL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_CREDIT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_ANNUITY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMT_GOODS_PRICE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_TYPE_SUITE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_INCOME_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_EDUCATION_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_FAMILY_STATUS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_HOUSING_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REGION_POPULATION_RELATIVE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_BIRTH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_EMPLOYED'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_REGISTRATION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAYS_ID_PUBLISH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OWN_CAR_AGE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_MOBIL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_WORK_PHONE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_PHONE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FLAG_EMAIL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OCCUPATION_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNT_FAM_MEMBERS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mapplication_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplication_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_fraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DataSet/application_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23eac821",
      "metadata": {
        "id": "23eac821"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OSyq7zKTOSLI"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}